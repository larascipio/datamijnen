{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from dateutil.parser import parse \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import as Dataframe\n",
    "df = pd.read_csv('./Data/dataset_mood_smartphone.csv')\n",
    "df.head()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "data =  df.drop(['Unnamed: 0'], axis=1)\n",
    "    \n",
    "# Make sure the 'time' column is of type datetime\n",
    "data['time'] = pd.to_datetime(data['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.datacleaning import remove_incorrect_values, convert_to_wide, DetectAnomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove incorrect values\n",
    "valid_df, removed_df = remove_incorrect_values(data)\n",
    "\n",
    "# Call the function and store the result in a new dataframe\n",
    "new_df = convert_to_wide(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the DetectAnomalies class\n",
    "anomaly_detector = DetectAnomalies(contamination=0.005)\n",
    "\n",
    "# Create a list of columns to be analyzed\n",
    "columns = ['activity', 'appCat.builtin', 'appCat.communication', 'appCat.entertainment', 'appCat.finance', 'appCat.game',\n",
    "'appCat.office', 'appCat.other', 'appCat.social', 'appCat.travel', 'appCat.unknown', 'appCat.utilities', 'appCat.weather', 'screen']\n",
    "    \n",
    "# Call the detect_anomalies method to identify anomalies in the data\n",
    "anomaly_df, clean_data, anomaly_dict = anomaly_detector.detect_anomalies(new_df, columns)\n",
    "\n",
    "# Plot the anomalies and non-anomalies for each column\n",
    "anomaly_detector.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the index title from the dataframe\n",
    "clean_data.index.name = None\n",
    "\n",
    "# Remove the missing values from the target variable\n",
    "clean_data = clean_data.dropna(subset=['mood']).reset_index(drop=True)\n",
    "\n",
    "# Check the number of missing values in the imputed data\n",
    "clean_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.datacleaning import impute_with0, ImputeKNN, ImputeIterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create an instance of TimeSeriesSplit with the number of splits\n",
    "tscv = TimeSeriesSplit(n_splits = 5) #\n",
    "\n",
    "for train_index, test_index in tscv.split(clean_data):\n",
    "    train_data = clean_data.iloc[train_index]\n",
    "    test_data = clean_data.iloc[test_index]\n",
    "\n",
    "\n",
    "print(train_data.shape) # (1015, 21)\n",
    "print(test_data.shape) # (202, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to impute\n",
    "cols_to_impute = [col for col in train_data.columns if col not in ['id', 'date', 'mood']]\n",
    "\n",
    "# Impute missing values with 0 for train and test data\n",
    "zero_train = impute_with0(train_data, cols_to_impute)\n",
    "zero_test = impute_with0(test_data, cols_to_impute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Iterative Imputataion class\n",
    "KNNimputer_train = ImputeKNN(train_data, cols_to_impute)\n",
    "KNNimputer_test = ImputeKNN(test_data, cols_to_impute)\n",
    "\n",
    "# Impute missing values and join the imputed data to the original DataFrame\n",
    "KNN_train = KNNimputer_train.impute()\n",
    "KNN_test = KNNimputer_test.impute()\n",
    "\n",
    "# Join with original data\n",
    "KNN_train_df = KNNimputer_train.join2full(train_data)\n",
    "KNN_test_df = KNNimputer_test.join2full(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the Iterative Imputataion class\n",
    "ITimputer_train = ImputeIterative(train_data, cols_to_impute)\n",
    "ITimputer_test = ImputeIterative(test_data, cols_to_impute)\n",
    "\n",
    "# Impute missing values and join the imputed data to the original DataFrame\n",
    "IT_train = ITimputer_train.impute()\n",
    "IT_test = ITimputer_test.impute()\n",
    "\n",
    "# Join with original data\n",
    "IT_train_df = ITimputer_train.join2full(train_data)\n",
    "IT_test_df = ITimputer_test.join2full(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocessing.featureengineering import feature_engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering zero data\n",
    "zero_train_fe = feature_engineering(zero_train)\n",
    "zero_test_fe = feature_engineering(zero_test)\n",
    "\n",
    "# Feature engineering KNN data\n",
    "KNN_train_fe = feature_engineering(KNN_train_df)\n",
    "KNN_test_fe = feature_engineering(KNN_test_df)\n",
    "\n",
    "# Feature engineering Iterative data\n",
    "IT_train_fe = feature_engineering(IT_train_df)\n",
    "IT_test_fe = feature_engineering(IT_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into features and target\n",
    "X_train_zero = zero_train_fe.drop(['id', 'date', 'mood'], axis=1)\n",
    "y_train_zero = zero_train_fe['mood']\n",
    "X_test_zero = zero_test_fe.drop(['id', 'date', 'mood'], axis=1)\n",
    "y_test_zero = zero_test_fe['mood']\n",
    "\n",
    "# Of zonder imputation\n",
    "X_train_zero = train_data.drop(['id', 'date', 'mood'], axis=1)\n",
    "y_train_zero = train_data['mood']\n",
    "X_test_zero = test_data.drop(['id', 'date', 'mood'], axis=1)\n",
    "y_test_zero = test_data['mood']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
